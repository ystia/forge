tosca_definitions_version: alien_dsl_2_0_0

#
# Ystia Forge
# Copyright (C) 2018 Bull S. A. S. - Bull, Rue Jean Jaures, B.P.68, 78340, Les Clayes-sous-Bois, France.
# Use of this source code is governed by Apache 2 LICENSE that can be found in the LICENSE file.
#

metadata:
  template_name: org.ystia.kafka.linux.bash
  template_version: 2.3.0-SNAPSHOT
  template_author: Ystia

description: Kafka is a distributed, partitioned, replicated commit log service.

imports:
  - tosca-normative-types:1.0.0-ALIEN20
  - org.ystia.common:2.3.0-SNAPSHOT
  - org.ystia.consul.pub:2.3.0-SNAPSHOT
  - org.ystia.java.pub:2.3.0-SNAPSHOT
  - org.ystia.kafka.pub:2.3.0-SNAPSHOT

node_types:
  org.ystia.kafka.linux.bash.nodes.Kafka:
    derived_from: org.ystia.consul.pub.nodes.ConsulUser
    description: Kafka component for linux
    tags:
      icon: /images/kafka-icon.png
    properties:
      component_version:
        type: version
        description: Version of the installed Kafka component
        default: 0.10.2.2
        constraints:
          - valid_values: [0.10.2.2, 0.11.0.3]
      scala_version:
        type: version
        description: Version of the scala imbedded
        default: 2.11
        constraints:
          - valid_values: [2.11, 2.12]
      repository:
        type: string
        description: >
          This property give the opportunity to specify an alternative download repository for this component artifacts.
          It is your responsibility to provide an accessible download url and to store required artifacts on it.
          You should specify only the base repository url. Artifacts names will be appended to it, so this property could be shared among
          several components using the inputs feature.
        required: true
        default: http://mirrors.standaloneinstaller.com/apache/
        constraints:
          - pattern: ^(http|https|ftp)://.+/.*$
      kf_heap_size:
        type: string
        default: "1G"
        description: >
          This property allows to set the heap memory size that is allocated to Kafka java process,
          It allocates the same value to both initial and maximum values (ie -Xms and -Xmx java options).
        constraints:
          - pattern: "[1-9][0-9]*[kKmMgG]"
      zk_heap_size:
        type: string
        default: "500M"
        description: >
          This property allows to set the heap memory size that is allocated to Zookeeper java process,
          It allocates the same value to both initial and maximum values (ie -Xms and -Xmx java options).
        constraints:
          - pattern: "[1-9][0-9]*[kKmMgG]"
      log_cleaner_enable:
        type: boolean
        default: false
        description: >
          This property allows you to enable the default Kafka log cleaner.
          The default value is false.
          The default policy for the cleaner is to delete the log segments older than 7 days.
    attributes:
      java_home: { get_operation_output: [ SELF, Configure, pre_configure_source, JAVA_HOME ] }
      kafka_home: { get_operation_output: [ SELF, Standard, create, KAFKA_HOME ] }
    capabilities:
      host:
        type: org.ystia.kafka.pub.capabilities.KafkaHosting
        occurrences: [0,unbounded]
    requirements:
      - host:
          capability: org.ystia.java.pub.capabilities.JavaHosting
          relationship: org.ystia.java.pub.relationships.HostedOnJavaRuntime
          occurrences: [1, 1]
      - filesystem_endpoint:
          capability: tosca.capabilities.Node
          relationship: org.ystia.kafka.linux.bash.relationships.ConnectsToFilessystem
          occurrences: [0,1]
    interfaces:
      Standard:
        create:
          inputs:
            REPOSITORY: { get_property: [SELF, repository] }
            KAFKA_VERSION: { get_property: [SELF, component_version] }
            SCALA_VERSION: { get_property: [SELF, scala_version] }
          implementation: scripts/kafka_install.sh
        configure:
          inputs:
            IP_ADDRESS: { get_attribute: [SELF, ip_address] }
            LOG_CLEANER_ENABLE: { get_property: [SELF, log_cleaner_enable] }
            JAVA_HOME: { get_attribute: [SELF, java_home] }
            KAFKA_HOME: { get_attribute: [SELF, kafka_home] }
            KF_HEAP_SIZE: { get_property: [SELF, kf_heap_size] }
            ZK_HEAP_SIZE: { get_property: [SELF, zk_heap_size] }
          implementation: scripts/kafka_configure.sh
        start:
          inputs:
            KAFKA_HOME: { get_attribute: [SELF, kafka_home] }
            JAVA_HOME: { get_attribute: [SELF, java_home] }
          implementation:  scripts/kafka_start.sh
        stop:
          inputs:
            KAFKA_HOME: { get_attribute: [SELF, kafka_home] }  
            JAVA_HOME: { get_attribute: [SELF, java_home] }
          implementation:  scripts/kafka_stop.sh
    artifacts:
      - scripts:
          file: scripts
          type: tosca.artifacts.File

  org.ystia.kafka.linux.bash.nodes.KafkaTopic:
    derived_from: org.ystia.nodes.Root
    description: Kafka topic implementation for linux
    tags:
      icon: /images/kafka-topic-icon.png
    properties:
      topic_name:
        type: string
        description: Name of this topic
        required: true
        constraints:
          - pattern: "[-_A-Za-z0-9]+"
      partitions:
        type: integer
        description: Number of partitions. default is 1 partition.
        required: false
        default: 1
      replicas:
        type: integer
        description: Number of replicas. default is 1 replica.
        required: false
        default: 1
      min_insync_replicas:
        description: >
          When a producer sets request_required_acks to in_syncs, min_insync_replicas specifies the minimum number of replicas
          that must acknowledge a write for the write to be considered successful.
          If this minimum cannot be met, then the producer will raise an exception (either NotEnoughReplicas or NotEnoughReplicasAfterAppend).
          When used together, min_insync_replicas and request_required_acks allow you to enforce greater durability guarantees.
          A typical scenario would be to create a topic with a replication factor of 3, set min_insync_replicas to 2,
          and produce with request_required_acks of in_syncs.
          This will ensure that the producer raises an exception if a majority of replicas do not receive a write.
        type: integer
        required: false
        default: 1
        constraints:
          - greater_or_equal: 0
      retention_minutes:
        description: >
          The number of minutes to keep a log file before deleting it.
          Default value is 7 days.
        type: integer
        required: false
        default: 10080
        constraints:
          - greater_or_equal: 1
      segment_minutes:
        description: >
          This configuration controls the period of time after which Kafka will force the log to roll
          even if the segment file isn't full to ensure that retention can delete or compact old data.
          Default value is 7 days.
        type: integer
        required: false
        default: 10080
        constraints:
          - greater_or_equal: 1
      segment_bytes:
        description: >
          Segment file size for the log.
          Default value is 1GB.
        type: integer
        required: false
        default: 1073741824
        constraints:
          - greater_or_equal: 1024
    capabilities:
      kafka_topic: org.ystia.kafka.pub.capabilities.KafkaTopic
      spark_app_resource: org.ystia.kafka.pub.capabilities.SparkEndpoint
    requirements:
      - host:
          capability: org.ystia.kafka.pub.capabilities.KafkaHosting
          relationship: org.ystia.kafka.linux.bash.relationships.HostedOnKafka
          occurrences: [1,1]
    interfaces:
      Standard:
        create:
          inputs:
            TOPIC_NAME: { get_property: [SELF, topic_name] }
            PARTITIONS: { get_property: [SELF, partitions] }
            REPLICAS: { get_property: [SELF, replicas] }
            MIN_INSYNC_REPLICAS: { get_property: [SELF, min_insync_replicas] }
            RETENTION_MINUTES: { get_property: [SELF, retention_minutes] }
            SEGMENT_BYTES: { get_property: [SELF, segment_bytes] }
            SEGMENT_MINUTES: { get_property: [SELF, segment_minutes] }
          implementation: scripts/createKafkaTopic.sh
    artifacts:
      - scripts:
          file: scripts
          type: tosca.artifacts.File

relationship_types:

  org.ystia.kafka.linux.bash.relationships.HostedOnKafka:
    derived_from: org.ystia.relationships.HostedOn
    description: Relationship between a Kafka topic and Kafka
    valid_target_types: [ org.ystia.kafka.linux.bash.nodes.Kafka ]

  org.ystia.kafka.linux.bash.relationships.ConnectsToFilessystem:
    derived_from: org.ystia.relationships.ConnectsTo
    description: Connects Kafka to Block Storage File System
    valid_target_types: [ tosca.capabilities.Node ]
    interfaces:
      Configure:
          pre_configure_source:
            inputs:
              path_fs: { get_property: [ TARGET, location ] }
            implementation: scripts/kafka-to-volume.sh
    artifacts:
      - scripts:
          file: scripts
          type: tosca.artifacts.File
